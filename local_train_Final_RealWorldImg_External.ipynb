{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Synthetic Data Generation and Training Workflow with Warehouse Sim Ready Assets\n",
    "\n",
    "This notebook is the second part of the SDG and Training Workflow. Here, we will be focusing on training an Object Detection Network with TAO toolkit\n",
    "\n",
    "A high level overview of the steps:\n",
    "* Pulling TAO Docker Container\n",
    "* Training Detectnet_v2 model with generated Synthetic Data \n",
    "* Visualizing Model Performance on Sample Real World Data\n",
    "\n",
    "`This notebook is very similar to the cloud training notebook, only mounted directories and paths for the docker containers are changed. The data, model and training, evaluation and inference steps are identical` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If Isaac Sim is installed locally, ensure that data generation is complete. Run the `generate_data.sh` script in this folder. Ensure the path to Isaac Sim is set correctly in the script (`ISAAC_SIM_PATH` corresponds to where Isaac Sim is installed locally on your workstation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "This notebook shows an example usecase of Object Detection using DetectNet_v2 in the Train Adapt Optimize (TAO) Toolkit. We will train the model with Synthetic Data generated previously.\n",
    "\n",
    "1. [Set up TAO via Docker container](#head-1)\n",
    "2. [Download Pretrained model](#head-2)\n",
    "3. [Convert Dataset to TFRecords for TAO](#head-3)\n",
    "4. [Provide training specification](#head-4)\n",
    "5. [Run TAO training](#head-5)\n",
    "6. [Evaluate trained model](#head-6)\n",
    "7. [Visualize Model Predictions on Real World Data](#head-7)\n",
    "8. [Next Steps](#head-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up TAO via Docker Container <a class=\"anchor\" id=\"head-1\"></a>\n",
    "\n",
    "* We will follow the pre-requisites section of [instructions](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html#running-tao-toolkit) for using TAO toolkit. Make sure that the pre-requisite steps are completed (installing `docker`, `nvidia container toolkit` and `docker login nvcr.io`)\n",
    "\n",
    "* The docker container being used for training will be pulled in the cells below, make sure you have completed the pre-requisite steps and `docker login nvcr.io` to allow pulling of the container from NGC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "%env DOCKER_REGISTRY=nvcr.io\n",
    "%env DOCKER_NAME=nvidia/tao/tao-toolkit\n",
    "%env DOCKER_TAG=4.0.0-tf1.15.5 ## for TensorFlow docker\n",
    "\n",
    "%env DOCKER_CONTAINER=nvcr.io/nvidia/tao/tao-toolkit:4.0.0-tf1.15.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Pretrained Model <a class=\"anchor\" id=\"head-2\"></a>\n",
    "\n",
    "* We will use the `detectnet_v2` Object Detection model with a `resnet18` backbone\n",
    "* Make sure the `LOCAL_PROJECT_DIR` environment variable has the path of this cloned repository in the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"LOCAL_PROJECT_DIR\"] = os.path.dirname(os.getcwd()) # This is the location of the root of the cloned repo\n",
    "print(os.environ[\"LOCAL_PROJECT_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget --quiet --show-progress --progress=bar:force:noscroll --auth-no-challenge --no-check-certificate \\\n",
    "        https://api.ngc.nvidia.com/v2/models/nvidia/tao/pretrained_detectnet_v2/versions/resnet18/files/resnet18.hdf5 \\\n",
    "        -P  $LOCAL_PROJECT_DIR/local/training/tao/pretrained_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $(readlink -f $LOCAL_PROJECT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Convert Dataset to TFRecords for TAO <a class=\"anchor\" id=\"head-3\"></a>\n",
    "\n",
    "* The `Detectnet_v2` model in TAO expects data in the form of TFRecords for training. \n",
    "* We can convert the KITTI Format Dataset generated from Part 1 with the `detectnet_v2 dataset_convert` tool provided with TAO toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Converting Tfrecords for palletjack warehouse distractors dataset\")\n",
    "\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/local/training/tao/tfrecords/distractors_warehouse && rm -rf $LOCAL_PROJECT_DIR/local/training/tao/tfrecords/distractors_warehouse/*\n",
    "\n",
    "!docker run -it --rm --gpus all -v $LOCAL_PROJECT_DIR:/workspace/tao-experiments $DOCKER_CONTAINER \\\n",
    "                   detectnet_v2 dataset_convert \\\n",
    "                  -d /workspace/tao-experiments/local/training/tao/specs/tfrecords/distractors_warehouse.txt \\\n",
    "                  -o /workspace/tao-experiments/local/training/tao/tfrecords/distractors_warehouse/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Converting Tfrecords for palletjack with additional distractors\")\n",
    "\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/local/training/tao/tfrecords/distractors_additional && rm -rf $LOCAL_PROJECT_DIR/local/training/tao/tfrecords/distractors_additional/*\n",
    "\n",
    "!docker run -it --rm --gpus all -v $LOCAL_PROJECT_DIR:/workspace/tao-experiments $DOCKER_CONTAINER \\\n",
    "                   detectnet_v2 dataset_convert \\\n",
    "                  -d /workspace/tao-experiments/local/training/tao/specs/tfrecords/distractors_additional.txt \\\n",
    "                  -o /workspace/tao-experiments/local/training/tao/tfrecords/distractors_additional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Converting Tfrecords for kitti trainval dataset\")\n",
    "# !mkdir -p $LOCAL_DATA_DIR/tfrecords/july/distractors_palletjack_warehouse && rm -rf $LOCAL_DATA_DIR/tfrecords/july/distractors_palletjack_warehouse/*\n",
    "!mkdir -p $LOCAL_PROJECT_DIR/local/training/tao/tfrecords/no_distractors && rm -rf $LOCAL_PROJECT_DIR/local/training/tao/tfrecords/no_distractors/*\n",
    "\n",
    "!docker run -it --rm --gpus all -v $LOCAL_PROJECT_DIR:/workspace/tao-experiments $DOCKER_CONTAINER \\\n",
    "                   detectnet_v2 dataset_convert \\\n",
    "                  -d /workspace/tao-experiments/local/training/tao/specs/tfrecords/no_distractors.txt \\\n",
    "                  -o /workspace/tao-experiments/local/training/tao/tfrecords/no_distractors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Provide Training Specification File <a class=\"anchor\" id=\"head-4\"></a>\n",
    "\n",
    "* The spec file for training with TAO is provided under `$LOCAL_PROJECT_DIR/specs/training/resnet18_distractors.txt`\n",
    "* The tfrecords and the synthetic data generated in the previous steps are provided under the `dataset_config` parameter of the file\n",
    "* Other parameters like `augmentation_config`, `model_config`, `postprocessing_config` can be adjusted. Refer to [this](https://docs.nvidia.com/tao/tao-toolkit/text/object_detection/detectnet_v2.html) for a detailed guideline on adjusting the parameters in the spec file\n",
    "* For training our model to detect `palletjacks` this `spec` file provided can be used directly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Refer to this for a detailed guideline on adjusting the parameters in the spec file\". -> the link is broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat $LOCAL_PROJECT_DIR/local/training/tao/specs/training/resnet18_distractors.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters can be set in the `spec` file. Adjust batch size parameter depending on the VRAM of your GPU \n",
    "\n",
    "* You can increase the number of epochs, the number of false positives in real world images keeps decreasing (mAP does not change much after ~250 epochs and usually results in the best trained model for the given dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run TAO Training <a class=\"anchor\" id=\"head-5\"></a>\n",
    "\n",
    "* The `$LOCAL_PROJECT_DIR` will be mounted to the TAO docker for training, this contains all the data, pretrained model and spec files (training and inference) needed\n",
    "\n",
    "#### Ensure that no `_warning.json` file exists in the `$LOCAL_PROJECT_DIR/cloud/training/tao/tfrecords` sub-folders (`distractors_additional`, `ditractors_warehouse` and `no_distractors`)\n",
    "* Delete the `_warning.json` files before beginning training\n",
    "* TAO training won't begin if the structure of the `tfrecords` folder directories is not as expected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up env variables for cleaner command line commands.\n",
    "%env KEY=tlt_encode\n",
    "%env NUM_GPUS=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* TAO Training can be stopped and resumed (`checkpoint_interval` parameter specified in the `spec` file)\n",
    "* Tensorboard visualization can be used with TAO [instructions](https://docs.nvidia.com/tao/tao-toolkit/text/tensorboard_visualization.html#visualizing-using-tensorboard). \n",
    "* The `$RESULTS_DIR` parameter is the folder where the `$LOCAL_PROJECT_DIR/local/training/tao/detectnet_v2/resnet18_palletjack` folder which is specified with the `-i` flag in the command below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete old log files from previous run (this wasn't in the documentation)\n",
    "!rm -rf $LOCAL_PROJECT_DIR/local/training/tao/detectnet_v2/resnet18_palletjack/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all -v $LOCAL_PROJECT_DIR:/workspace/tao-experiments $DOCKER_CONTAINER \\\n",
    "            detectnet_v2 train -e /workspace/tao-experiments/local/training/tao/specs/training/resnet18_distractors.txt \\\n",
    "            -r /workspace/tao-experiments/local/training/tao/detectnet_v2/resnet18_palletjack -k $KEY --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Trained Model <a class=\"anchor\" id=\"head-6\"></a>\n",
    "\n",
    "* While generating the `tfrecords` part of the total data generated was kept as a validation set (14% of total data)\n",
    "* We will run our model evaluation on this data to obtain metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all -v $LOCAL_PROJECT_DIR:/workspace/tao-experiments $DOCKER_CONTAINER \\\n",
    "            detectnet_v2 evaluate -e /workspace/tao-experiments/local/training/tao/specs/training/resnet18_distractors.txt \\\n",
    "            -m /workspace/tao-experiments/local/training/tao/detectnet_v2/resnet18_palletjack/weights/model.tlt \\\n",
    "            -k $KEY --gpus $NUM_GPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Performance on Real World Data <a class=\"anchor\" id=\"head-7\"></a>\n",
    "\n",
    "* Lets visualize the model predictions on a few sample real world images next\n",
    "* We will use palletjack images in a warehouse from the `LOCO` dataset to understand if the model is capable of performing real world detections\n",
    "* Additional images can be placed under the `loco_palletjacks` folder of this project. The input folder is specified with the `-i` flag in the command below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker run -it --rm --gpus all -v $LOCAL_PROJECT_DIR:/workspace/tao-experiments $DOCKER_CONTAINER \\\n",
    "detectnet_v2 inference -e /workspace/tao-experiments/local/training/tao/specs/inference/new_inference_specs.txt \\\n",
    "-o /workspace/tao-experiments/local/training/tao/detectnet_v2/resnet18_palletjack/loco_results \\\n",
    "-i /workspace/tao-experiments/images/loco_palletjacks \\\n",
    "-k $KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image \n",
    "import glob\n",
    "\n",
    "# Fetch results from real-world data.\n",
    "# replace the wrong folder name with the correct folder name \"loco_results\"\n",
    "# results_dir = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], \"local/training/tao/detectnet_v2/resnet18_palletjack/test_loco/images_annotated\")\n",
    "results_dir = os.path.join(os.environ[\"LOCAL_PROJECT_DIR\"], \"local/training/tao/detectnet_v2/resnet18_palletjack/loco_results/images_annotated\")\n",
    "\n",
    "# 2. Discover all image correct image file names using their type (jpg, jpeg, png)\n",
    "image_names = sorted(\n",
    "    [os.path.basename(p) for p in glob.glob(os.path.join(results_dir, \"*.jpg\"))]\n",
    "    + [os.path.basename(p) for p in glob.glob(os.path.join(results_dir, \"*.jpeg\"))]\n",
    "    + [os.path.basename(p) for p in glob.glob(os.path.join(results_dir, \"*.png\"))]\n",
    ")\n",
    "\n",
    "images = [Image(filename = os.path.join(results_dir, image_name)) for image_name in image_names]\n",
    "\n",
    "print(f\"Found {len(image_names)} images\")\n",
    "\n",
    "display(*images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps <a class=\"anchor\" id=\"head-8\"></a>\n",
    "\n",
    "#### Generating Synthetic Data for your use case:\n",
    "\n",
    "* Make changes in the Domain Randomization under the Synthetic Data Generation script (`palletjack_sdg/standalone_palletjack_sdg.py`\n",
    "* Add additional objects of interest in the scene (similar to how `palletjacks` are added, you can add `forklifts`, `ladders` etc.) to generate data\n",
    "* Use [different](https://docs.nvidia.com/tao/tao-toolkit/text/tao_toolkit_quick_start_guide.html#downloading-the-models) models for training with TAO (for object detection, you can use `YOLO`, `SSD`, `EfficientDet`) \n",
    "* Replicator provides Semantic Segmentation, Instance Segmentation, Depth and various other ground truth annotations along with RGB. You can also write your own ground truth annotator (eg: Pose Estimation: Refer to [sample](https://docs.omniverse.nvidia.com/isaacsim/latest/tutorial_replicator_offline_pose_estimation.html). These can be used for training a model of your own framework and choice\n",
    "* Exploring the option of using `Synthetic + Real` data for training a network. Can be particularly useful for generating more data around particular corner cases\n",
    "\n",
    "\n",
    "#### Deploying Trained Models:\n",
    "\n",
    "* After obtaining satisfactory results with the training process, you can further optimize your model for deployment with the help of Pruning and QAT.\n",
    "* TAO models can directly be deployed on Jetson with Isaac ROS or Deepstream which ensures your end-to-end pipeline being optimized (data acquisition -> model inference -> results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "f23a2831654361cfd8b219e05b5055fdda3e37fe5c0b020e6226f740844c300a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
